{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import yfinance as yf\n!pip install ta\nfrom ta import add_all_ta_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:20.689750Z","iopub.execute_input":"2025-10-25T04:17:20.690794Z","iopub.status.idle":"2025-10-25T04:17:24.445859Z","shell.execute_reply.started":"2025-10-25T04:17:20.690752Z","shell.execute_reply":"2025-10-25T04:17:24.444784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import (\n    accuracy_score, \n    classification_report, \n    confusion_matrix,\n    roc_auc_score,\n    roc_curve,\n    ConfusionMatrixDisplay\n)\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:24.447599Z","iopub.execute_input":"2025-10-25T04:17:24.447885Z","iopub.status.idle":"2025-10-25T04:17:24.481930Z","shell.execute_reply.started":"2025-10-25T04:17:24.447862Z","shell.execute_reply":"2025-10-25T04:17:24.480959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:24.485101Z","iopub.execute_input":"2025-10-25T04:17:24.485413Z","iopub.status.idle":"2025-10-25T04:17:24.501353Z","shell.execute_reply.started":"2025-10-25T04:17:24.485389Z","shell.execute_reply":"2025-10-25T04:17:24.500507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using Apple (AAPL) as our example stock\nticker = 'AAPL'\nperiod = '2y'\n\nprint(f\"Fetching data for {ticker}...\")\ndata = yf.download(ticker, period=period, interval='1d', progress=False)\n\n# Get market context (S&P 500 and VIX)\nspx = yf.download(\"^GSPC\", period=period, interval='1d', progress=False)['Close']\nvix = yf.download(\"^VIX\", period=period, interval='1d', progress=False)['Close']\n\nif isinstance(data.columns, pd.MultiIndex):\n    data.columns = data.columns.get_level_values(0)\n\nif len(data.columns) == 5:\n    data.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n\nprint(f\"Downloaded {len(data)} days of data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:24.503215Z","iopub.execute_input":"2025-10-25T04:17:24.503492Z","iopub.status.idle":"2025-10-25T04:17:24.909984Z","shell.execute_reply.started":"2025-10-25T04:17:24.503472Z","shell.execute_reply":"2025-10-25T04:17:24.909158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n    if isinstance(data[col].iloc[0], (np.ndarray, list)):\n        data[col] = data[col].apply(lambda x: x[0])\n\ndata = add_all_ta_features(\n    data, \n    open=\"Open\", \n    high=\"High\", \n    low=\"Low\", \n    close=\"Close\", \n    volume=\"Volume\",\n    fillna=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:24.910811Z","iopub.execute_input":"2025-10-25T04:17:24.911099Z","iopub.status.idle":"2025-10-25T04:17:25.137911Z","shell.execute_reply.started":"2025-10-25T04:17:24.911078Z","shell.execute_reply":"2025-10-25T04:17:25.136999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multi-period returns\ndata['Return_1d'] = data['Close'].pct_change()\ndata['Return_3d'] = data['Close'].pct_change(3)\ndata['Return_5d'] = data['Close'].pct_change(5)\ndata['Return_10d'] = data['Close'].pct_change(10)\n\n# Moving averages\ndata['MA_5'] = data['Close'].rolling(5).mean()\ndata['MA_10'] = data['Close'].rolling(10).mean()\ndata['MA_20'] = data['Close'].rolling(20).mean()\ndata['MA_50'] = data['Close'].rolling(50).mean()\n\n# Volatility regimes\ndata['Vol_5d'] = data['Return_1d'].rolling(5).std()\ndata['Vol_10d'] = data['Return_1d'].rolling(10).std()\ndata['Vol_20d'] = data['Return_1d'].rolling(20).std()\ndata['Volatility_Regime'] = (data['Vol_10d'] > data['Vol_10d'].median()).astype(int)\n\n# Price relative features\ndata['Close_to_High'] = data['Close'] / data['High']\ndata['Close_to_Low'] = data['Close'] / data['Low']\ndata['High_Low_Range'] = (data['High'] - data['Low']) / data['Close']\n\n# Volume analysis\ndata['Volume_Change'] = data['Volume'].pct_change()\ndata['Vol_MA_5'] = data['Volume'].rolling(5).mean()\ndata['Vol_MA_20'] = data['Volume'].rolling(20).mean()\ndata['Volume_Ratio'] = data['Volume'] / data['Vol_MA_20']\ndata['Vol_Spike'] = (data['Volume'] / data['Volume'].rolling(10).mean() > 1.5).astype(int)\n\n# Price gaps\ndata['Gap'] = (data['Open'] - data['Close'].shift(1)) / data['Close'].shift(1)\ndata['Gap_Up'] = (data['Gap'] > 0.005).astype(int)\ndata['Gap_Down'] = (data['Gap'] < -0.005).astype(int)\n\n# MA crossovers\ndata['MA5_MA20_Cross'] = (data['MA_5'] > data['MA_20']).astype(int)\ndata['MA10_MA50_Cross'] = (data['MA_10'] > data['MA_50']).astype(int)\ndata['Price_Above_MA50'] = (data['Close'] > data['MA_50']).astype(int)\n\n# Momentum indicators\ndata['Momentum_3d'] = data['Close'] / data['Close'].shift(3) - 1\ndata['Momentum_5d'] = data['Close'] / data['Close'].shift(5) - 1\ndata['Momentum_10d'] = data['Close'] / data['Close'].shift(10) - 1\n\n# Candlestick patterns\ndata['Bullish_Engulfing'] = (\n    (data['Close'] > data['Open']) & \n    (data['Close'].shift(1) < data['Open'].shift(1)) &\n    (data['Close'] > data['Open'].shift(1)) &\n    (data['Open'] < data['Close'].shift(1))\n).astype(int)\n\ndata['Bearish_Engulfing'] = (\n    (data['Close'] < data['Open']) & \n    (data['Close'].shift(1) > data['Open'].shift(1)) &\n    (data['Close'] < data['Open'].shift(1)) &\n    (data['Open'] > data['Close'].shift(1))\n).astype(int)\n\ndata['Doji'] = (\n    abs(data['Close'] - data['Open']) / (data['High'] - data['Low']) < 0.1\n).astype(int)\n\n# External market context - align indices first\nspx_aligned = spx.reindex(data.index, method='ffill')\nvix_aligned = vix.reindex(data.index, method='ffill')\n\ndata['SPX_Return'] = spx_aligned.pct_change()\nspx_ma_5 = spx_aligned.rolling(5).mean()\ndata['SPX_MA_5'] = spx_ma_5\ndata['Market_Trend'] = (spx_aligned > spx_ma_5).astype(int)\ndata['VIX'] = vix_aligned\nvix_ma_20 = vix_aligned.rolling(20).mean()\ndata['VIX_High'] = (vix_aligned > vix_ma_20).astype(int)\n\n# Lag features for temporal patterns\nlag_features = ['Return_1d', 'momentum_rsi', 'trend_macd', 'Volume', 'Vol_10d']\nfor feat in lag_features:\n    if feat in data.columns:\n        for lag in [1, 2, 3, 5]:\n            data[f'{feat}_lag{lag}'] = data[feat].shift(lag)\n\n# Price position indicators\nif 'trend_sma_fast' in data.columns:\n    data['Price_vs_SMA20'] = data['Close'] / data['trend_sma_fast']\nif 'trend_sma_slow' in data.columns:\n    data['Price_vs_SMA50'] = data['Close'] / data['trend_sma_slow']\n\n# Bollinger Bands position\nif 'volatility_bbh' in data.columns and 'volatility_bbl' in data.columns:\n    bb_width = data['volatility_bbh'] - data['volatility_bbl']\n    data['BB_Position'] = (data['Close'] - data['volatility_bbl']) / bb_width\n    data['BB_Squeeze'] = (bb_width / data['Close'] < 0.02).astype(int)\n\n# RSI levels\nif 'momentum_rsi' in data.columns:\n    data['RSI_Overbought'] = (data['momentum_rsi'] > 70).astype(int)\n    data['RSI_Oversold'] = (data['momentum_rsi'] < 30).astype(int)\n    data['RSI_Neutral'] = ((data['momentum_rsi'] >= 45) & (data['momentum_rsi'] <= 55)).astype(int)\n\ndata.dropna(inplace=True)\nprint(f\"Feature engineering complete: {data.shape[1]} features\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:25.138778Z","iopub.execute_input":"2025-10-25T04:17:25.139014Z","iopub.status.idle":"2025-10-25T04:17:25.214651Z","shell.execute_reply.started":"2025-10-25T04:17:25.138995Z","shell.execute_reply":"2025-10-25T04:17:25.213760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['Future_Return_3d'] = data['Close'].shift(-3) / data['Close'] - 1\ndata['Target'] = np.where(\n    data['Future_Return_3d'] > 0.01, 1,\n    np.where(data['Future_Return_3d'] < -0.01, 0, np.nan)\n)\ndata.dropna(inplace=True)\n\nprint(f\"\\nTarget distribution (3-day return > 1%):\")\nprint(data['Target'].value_counts())\nprint(f\"Final dataset: {len(data)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:25.215675Z","iopub.execute_input":"2025-10-25T04:17:25.216294Z","iopub.status.idle":"2025-10-25T04:17:25.235535Z","shell.execute_reply.started":"2025-10-25T04:17:25.216262Z","shell.execute_reply":"2025-10-25T04:17:25.234634Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exclude_cols = ['Target', 'Adj Close', 'Future_Return_3d']\nfeature_cols = [col for col in data.columns if col not in exclude_cols]\n\nX = data[feature_cols]\ny = data['Target']\n\n# Remove low variance features\nselector = VarianceThreshold(threshold=0.001)\nX_selected = selector.fit_transform(X)\nselected_features = X.columns[selector.get_support()].tolist()\n\nprint(f\"After variance threshold: {len(selected_features)} features\")\n\n# Remove highly correlated features\nX_temp = pd.DataFrame(X_selected, columns=selected_features)\ncorr_matrix = X_temp.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.9)]\nX_temp.drop(columns=to_drop, inplace=True)\n\nprint(f\"After correlation filter: {X_temp.shape[1]} features\")\n\n# Feature importance ranking with Random Forest\nrf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=5)\nrf_selector.fit(X_temp, y)\nimportances = pd.Series(rf_selector.feature_importances_, index=X_temp.columns)\ntop_features = importances.nlargest(40).index.tolist()\n\nX_final = X_temp[top_features]\nprint(f\"Using top 40 features based on RF importance\\n\")\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_final)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:25.236507Z","iopub.execute_input":"2025-10-25T04:17:25.236837Z","iopub.status.idle":"2025-10-25T04:17:25.561203Z","shell.execute_reply.started":"2025-10-25T04:17:25.236813Z","shell.execute_reply":"2025-10-25T04:17:25.560439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Walk-forward validation with 5 splits\ntscv = TimeSeriesSplit(n_splits=5)\ncv_scores = []\n\nprint(\"=\" * 70)\nprint(\"WALK-FORWARD CROSS-VALIDATION\")\nprint(\"=\" * 70)\n\nfold = 1\nfor train_idx, val_idx in tscv.split(X_scaled):\n    X_train_cv, X_val_cv = X_scaled[train_idx], X_scaled[val_idx]\n    y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]\n    \n    # Quick XGBoost validation\n    xgb_cv = XGBClassifier(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=4,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42\n    )\n    xgb_cv.fit(X_train_cv, y_train_cv)\n    val_pred = xgb_cv.predict(X_val_cv)\n    val_acc = accuracy_score(y_val_cv, val_pred)\n    cv_scores.append(val_acc)\n    \n    print(f\"Fold {fold}: Val Accuracy = {val_acc:.4f}\")\n    fold += 1\n\nprint(f\"\\nMean CV Accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\nprint(\"=\" * 70 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:25.562040Z","iopub.execute_input":"2025-10-25T04:17:25.562388Z","iopub.status.idle":"2025-10-25T04:17:27.067160Z","shell.execute_reply.started":"2025-10-25T04:17:25.562367Z","shell.execute_reply":"2025-10-25T04:17:27.066297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_idx = int(len(X_scaled) * 0.8)\nX_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\nprint(f\"Training samples: {len(X_train)}\")\nprint(f\"Test samples: {len(X_test)}\\n\")\n\n# Class weights\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\nclass_weight_dict = {i: w for i, w in enumerate(class_weights)}\n\n# Add Gaussian noise for regularization\nnoise = np.random.normal(0, 0.01, X_train.shape)\nX_train_noisy = X_train + noise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:27.069827Z","iopub.execute_input":"2025-10-25T04:17:27.070091Z","iopub.status.idle":"2025-10-25T04:17:27.078179Z","shell.execute_reply.started":"2025-10-25T04:17:27.070069Z","shell.execute_reply":"2025-10-25T04:17:27.077368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training XGBoost model\")\nxgb_model = XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric='logloss',\n    random_state=42\n)\nxgb_model.fit(X_train_noisy, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:27.079231Z","iopub.execute_input":"2025-10-25T04:17:27.079510Z","iopub.status.idle":"2025-10-25T04:17:28.406899Z","shell.execute_reply.started":"2025-10-25T04:17:27.079478Z","shell.execute_reply":"2025-10-25T04:17:28.406097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training LightGBM model\")\nlgb_model = LGBMClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42,\n    verbose=-1\n)\nlgb_model.fit(X_train_noisy, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:28.407682Z","iopub.execute_input":"2025-10-25T04:17:28.408001Z","iopub.status.idle":"2025-10-25T04:17:28.559287Z","shell.execute_reply.started":"2025-10-25T04:17:28.407979Z","shell.execute_reply":"2025-10-25T04:17:28.558443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Building neural network\")\nnn_model = Sequential([\n    Dense(128, input_dim=X_train.shape[1], kernel_regularizer=l2(0.001)),\n    LeakyReLU(alpha=0.1),\n    BatchNormalization(),\n    Dropout(0.4),\n    \n    Dense(64, kernel_regularizer=l2(0.001)),\n    LeakyReLU(alpha=0.1),\n    BatchNormalization(),\n    Dropout(0.3),\n    \n    Dense(32, kernel_regularizer=l2(0.001)),\n    LeakyReLU(alpha=0.1),\n    Dropout(0.2),\n    \n    BatchNormalization(),\n    Dense(1, activation='sigmoid')\n])\n\nnn_model.compile(\n    optimizer=Adam(learning_rate=0.001, decay=1e-5),\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True,\n    verbose=0\n)\n\nlr_reduce = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=7,\n    min_lr=1e-6,\n    verbose=0\n)\n\nprint(\"Training neural network\")\nhistory = nn_model.fit(\n    X_train_noisy, y_train,\n    epochs=150,\n    batch_size=16,\n    validation_split=0.15,\n    class_weight=class_weight_dict,\n    callbacks=[early_stop, lr_reduce],\n    verbose=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:28.560225Z","iopub.execute_input":"2025-10-25T04:17:28.560491Z","iopub.status.idle":"2025-10-25T04:17:37.493513Z","shell.execute_reply.started":"2025-10-25T04:17:28.560464Z","shell.execute_reply":"2025-10-25T04:17:37.492857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Building stacking ensemble\")\nstack_model = StackingClassifier(\n    estimators=[\n        ('xgb', xgb_model),\n        ('lgb', lgb_model),\n        ('rf', RandomForestClassifier(n_estimators=200, max_depth=6, random_state=42))\n    ],\n    final_estimator=LogisticRegression(),\n    cv=3,\n    n_jobs=-1\n)\nstack_model.fit(X_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:37.494915Z","iopub.execute_input":"2025-10-25T04:17:37.495228Z","iopub.status.idle":"2025-10-25T04:17:44.121117Z","shell.execute_reply.started":"2025-10-25T04:17:37.495202Z","shell.execute_reply":"2025-10-25T04:17:44.120408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 70)\nprint(\"MODEL PERFORMANCE COMPARISON\")\nprint(\"=\" * 70)\n\nmodels = {\n    'XGBoost': xgb_model,\n    'LightGBM': lgb_model,\n    'Neural Network': nn_model,\n    'Stacking Ensemble': stack_model\n}\n\nresults = {}\nfor name, model in models.items():\n    if name == 'Neural Network':\n        y_pred_proba = model.predict(X_test, verbose=0)\n        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n    else:\n        y_pred = model.predict(X_test)\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n    \n    acc = accuracy_score(y_test, y_pred)\n    try:\n        auc = roc_auc_score(y_test, y_pred_proba)\n    except:\n        auc = 0.0\n    \n    results[name] = {'accuracy': acc, 'roc_auc': auc, 'predictions': y_pred}\n    \n    print(f\"\\n{name}:\")\n    print(f\"  Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n    print(f\"  ROC-AUC:  {auc:.4f}\")\n\n# Find best model\nbest_model_name = max(results, key=lambda x: results[x]['roc_auc'])\nbest_predictions = results[best_model_name]['predictions']\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"BEST MODEL: {best_model_name}\")\nprint(f\"{'=' * 70}\\n\")\n\n# Detailed report for best model\nprint(classification_report(y_test, best_predictions, target_names=['DOWN', 'UP']))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, best_predictions)\ndisp = ConfusionMatrixDisplay(cm, display_labels=['DOWN', 'UP'])\ndisp.plot(cmap='Blues', values_format='d')\nplt.title(f'Confusion Matrix - {best_model_name}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:44.123370Z","iopub.execute_input":"2025-10-25T04:17:44.124667Z","iopub.status.idle":"2025-10-25T04:17:44.742756Z","shell.execute_reply.started":"2025-10-25T04:17:44.124639Z","shell.execute_reply":"2025-10-25T04:17:44.741975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\" * 70)\nprint(\"CLASS DISTRIBUTION ANALYSIS\")\nprint(\"=\" * 70)\ntest_data = data.iloc[split_idx:split_idx + len(y_test)].copy()\n\n# Check training distribution\ntrain_dist = y_train.value_counts()\nprint(\"\\nTraining Set:\")\nprint(train_dist)\nprint(f\"DOWN: {train_dist[0]/len(y_train)*100:.1f}%\")\nprint(f\"UP:   {train_dist[1]/len(y_train)*100:.1f}%\")\n\n# Check test distribution\ntest_dist = y_test.value_counts()\nprint(\"\\nTest Set:\")\nprint(test_dist)\nprint(f\"DOWN: {test_dist[0]/len(y_test)*100:.1f}%\")\nprint(f\"UP:   {test_dist[1]/len(y_test)*100:.1f}%\")\n\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PREDICTION ANALYSIS\")\nprint(\"=\" * 70)\n\n# Get probabilities from LightGBM (your best model)\ny_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\ntest_data['Prediction'] = (y_pred_proba_lgb > 0.086).astype(int)  # ← OPTIMAL THRESHOLD\n\ntest_data['Actual_Return_3d'] = test_data['Future_Return_3d']\ntest_data['Strategy_Return'] = np.where(\n    test_data['Prediction'] == 1,\n    test_data['Actual_Return_3d'],\n    0\n)\n\n# What is the model actually predicting?\npred_dist = pd.Series(y_pred_default).value_counts()\nprint(\"\\nDefault Predictions (threshold=0.5):\")\nprint(pred_dist)\nprint(f\"Predicting DOWN: {pred_dist.get(0, 0)/len(y_pred_default)*100:.1f}%\")\nprint(f\"Predicting UP:   {pred_dist.get(1, 0)/len(y_pred_default)*100:.1f}%\")\n\n# Check probability distribution\nprint(f\"\\nProbability Statistics:\")\nprint(f\"Mean probability: {y_pred_proba.mean():.3f}\")\nprint(f\"Median probability: {np.median(y_pred_proba):.3f}\")\nprint(f\"Probabilities > 0.5: {(y_pred_proba > 0.5).sum()} ({(y_pred_proba > 0.5).mean()*100:.1f}%)\")\n\n#FIND OPTIMAL THRESHOLD\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"THRESHOLD OPTIMIZATION\")\nprint(\"=\" * 70)\n\n# Calculate precision-recall curve\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\n# Calculate F1 scores for each threshold\nf1_scores = 2 * (precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1] + 1e-10)\n\n# Find optimal threshold (maximizes F1)\noptimal_idx = np.argmax(f1_scores)\noptimal_threshold = thresholds[optimal_idx]\n\nprint(f\"\\nOptimal threshold: {optimal_threshold:.3f}\")\nprint(f\"This maximizes F1 score: {f1_scores[optimal_idx]:.3f}\")\n\n# Try multiple thresholds\nprint(\"\\nPerformance at different thresholds:\")\nprint(\"-\" * 70)\nprint(f\"{'Threshold':<12} {'Accuracy':<12} {'F1 Score':<12} {'UP Recall':<12}\")\nprint(\"-\" * 70)\n\nfor threshold in [0.3, 0.4, optimal_threshold, 0.5, 0.6]:\n    y_pred_thresh = (y_pred_proba > threshold).astype(int)\n    acc = (y_pred_thresh == y_test).mean()\n    f1 = f1_score(y_test, y_pred_thresh)\n    \n    # Calculate recall for UP class specifically\n    up_mask = y_test == 1\n    up_recall = (y_pred_thresh[up_mask] == 1).mean() if up_mask.sum() > 0 else 0\n    \n    marker = \" ← OPTIMAL\" if abs(threshold - optimal_threshold) < 0.01 else \"\"\n    print(f\"{threshold:<12.3f} {acc:<12.3f} {f1:<12.3f} {up_recall:<12.3f}{marker}\")\n\n# ============================================================================\n# STEP 4: APPLY OPTIMAL THRESHOLD\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS WITH OPTIMAL THRESHOLD\")\nprint(\"=\" * 70)\n\ny_pred_optimal = (y_pred_proba > optimal_threshold).astype(int)\n\nprint(f\"\\nClassification Report (threshold = {optimal_threshold:.3f}):\")\nprint(classification_report(y_test, y_pred_optimal, target_names=['DOWN', 'UP']))\n\n# Compare to default\nprint(f\"\\nClassification Report (default threshold = 0.086):\")\nprint(classification_report(y_test, y_pred_default, target_names=['DOWN', 'UP']))\n\n# ============================================================================\n# STEP 5: VISUALIZE THE IMPROVEMENT\n# ============================================================================\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\n# 1. Probability Distribution\naxes[0, 0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.6, label='Actual DOWN', color='red')\naxes[0, 0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.6, label='Actual UP', color='green')\naxes[0, 0].axvline(0.5, color='blue', linestyle='--', linewidth=2, label='Default (0.5)')\naxes[0, 0].axvline(optimal_threshold, color='orange', linestyle='--', linewidth=2, label=f'Optimal ({optimal_threshold:.2f})')\naxes[0, 0].set_xlabel('Predicted Probability')\naxes[0, 0].set_ylabel('Count')\naxes[0, 0].set_title('Prediction Probability Distribution')\naxes[0, 0].legend()\naxes[0, 0].grid(alpha=0.3)\n\n# 2. Precision-Recall Curve\naxes[0, 1].plot(recalls[:-1], precisions[:-1], linewidth=2, color='blue')\naxes[0, 1].scatter(recalls[optimal_idx], precisions[optimal_idx], \n                   s=200, c='red', marker='*', zorder=5, label=f'Optimal (F1={f1_scores[optimal_idx]:.3f})')\naxes[0, 1].set_xlabel('Recall')\naxes[0, 1].set_ylabel('Precision')\naxes[0, 1].set_title('Precision-Recall Curve')\naxes[0, 1].legend()\naxes[0, 1].grid(alpha=0.3)\n\n# 3. F1 Score vs Threshold\naxes[0, 2].plot(thresholds, f1_scores, linewidth=2, color='green')\naxes[0, 2].axvline(optimal_threshold, color='red', linestyle='--', linewidth=2)\naxes[0, 2].set_xlabel('Threshold')\naxes[0, 2].set_ylabel('F1 Score')\naxes[0, 2].set_title('F1 Score vs Threshold')\naxes[0, 2].grid(alpha=0.3)\n\n# 4. Confusion Matrix - Default\ncm_default = confusion_matrix(y_test, y_pred_default)\nsns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'],\n            ax=axes[1, 0])\naxes[1, 0].set_title('Confusion Matrix - Default (0.5)')\naxes[1, 0].set_ylabel('True Label')\naxes[1, 0].set_xlabel('Predicted Label')\n\n# 5. Confusion Matrix - Optimal\ncm_optimal = confusion_matrix(y_test, y_pred_optimal)\nsns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Greens',\n            xticklabels=['DOWN', 'UP'], yticklabels=['DOWN', 'UP'],\n            ax=axes[1, 1])\naxes[1, 1].set_title(f'Confusion Matrix - Optimal ({optimal_threshold:.2f})')\naxes[1, 1].set_ylabel('True Label')\naxes[1, 1].set_xlabel('Predicted Label')\n\n# 6. Metrics Comparison\nmetrics = {\n    'Accuracy': [\n        (y_pred_default == y_test).mean(),\n        (y_pred_optimal == y_test).mean()\n    ],\n    'F1 Score': [\n        f1_score(y_test, y_pred_default),\n        f1_score(y_test, y_pred_optimal)\n    ],\n    'UP Recall': [\n        (y_pred_default[y_test == 1] == 1).mean(),\n        (y_pred_optimal[y_test == 1] == 1).mean()\n    ]\n}\n\nx = np.arange(len(metrics))\nwidth = 0.35\nbars1 = axes[1, 2].bar(x - width/2, [metrics[m][0] for m in metrics], width, label='Default (0.5)', color='lightblue')\nbars2 = axes[1, 2].bar(x + width/2, [metrics[m][1] for m in metrics], width, label=f'Optimal ({optimal_threshold:.2f})', color='lightgreen')\n\naxes[1, 2].set_ylabel('Score')\naxes[1, 2].set_title('Metric Comparison')\naxes[1, 2].set_xticks(x)\naxes[1, 2].set_xticklabels(metrics.keys())\naxes[1, 2].legend()\naxes[1, 2].grid(alpha=0.3, axis='y')\naxes[1, 2].set_ylim([0, 1])\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        axes[1, 2].text(bar.get_x() + bar.get_width()/2., height,\n                       f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:44.743664Z","iopub.execute_input":"2025-10-25T04:17:44.743966Z","iopub.status.idle":"2025-10-25T04:17:47.447788Z","shell.execute_reply.started":"2025-10-25T04:17:44.743946Z","shell.execute_reply":"2025-10-25T04:17:47.446899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 70)\nprint(\"TRADING STRATEGY SIMULATION\")\nprint(\"=\" * 70)\n\n# Align predictions with test data\ntest_data = data.iloc[split_idx:split_idx + len(y_test)]\ntest_data = test_data.copy()\ntest_data['Prediction'] = best_predictions\ntest_data['Actual_Return'] = test_data['Close'].pct_change()\n\n# Strategy: Go long when predicting UP (1), flat otherwise\ntest_data['Strategy_Return'] = np.where(\n    test_data['Prediction'] == 1,\n    test_data['Actual_Return'],\n    0\n)\n\n# Calculate cumulative returns\ncumulative_market = (1 + test_data['Actual_Return']).cumprod() - 1\ncumulative_strategy = (1 + test_data['Strategy_Return']).cumprod() - 1\n\nprint(f\"\\nBuy & Hold Return: {cumulative_market.iloc[-1]:.2%}\")\nprint(f\"Strategy Return:   {cumulative_strategy.iloc[-1]:.2%}\")\nprint(f\"Outperformance:    {(cumulative_strategy.iloc[-1] - cumulative_market.iloc[-1]):.2%}%\")\n\n# Plot cumulative returns\nplt.figure(figsize=(12, 6))\nplt.plot(cumulative_market.values, label='Buy & Hold', linewidth=2)\nplt.plot(cumulative_strategy.values, label='ML Strategy', linewidth=2)\nplt.xlabel('Trading Days')\nplt.ylabel('Cumulative Return')\nplt.title(f'Strategy Performance - {best_model_name}')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Feature importance (from best tree model)\nif best_model_name in ['XGBoost', 'LightGBM']:\n    best_tree_model = models[best_model_name]\n    feat_imp = pd.Series(\n        best_tree_model.feature_importances_,\n        index=X_final.columns\n    ).nlargest(15)\n    \n    plt.figure(figsize=(10, 6))\n    feat_imp.plot(kind='barh')\n    plt.xlabel('Importance')\n    plt.title(f'Top 15 Features - {best_model_name}')\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ANALYSIS COMPLETE\")\nprint(\"=\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:17:47.448664Z","iopub.execute_input":"2025-10-25T04:17:47.448930Z","iopub.status.idle":"2025-10-25T04:17:48.078860Z","shell.execute_reply.started":"2025-10-25T04:17:47.448910Z","shell.execute_reply":"2025-10-25T04:17:48.077963Z"}},"outputs":[],"execution_count":null}]}